# Deep Learning

## Overview
* [稀疏自编码器](#ch1)
    * [神经网络](#ch1.1)
    * [反向传导算法](#ch1.2)
    * [梯度校验与稀疏性](#ch1.3)
* [矢量化编程实现](#ch2)

<h2 id="ch1">稀疏自编码器</h2>

<h3 id="ch1.1">神经网络</h3>
* 概述
    * 概念：一个神经元，N个输入值，经过激活函数，得到一个输出值。
    * 激活函数：sigmod函数-->激活函数，则神经元为一个逻辑回归。
    * 如何选择激活函数：通常选择一些具有两个极值，且从一个极值能平滑过渡到另一个极值的函数。通常选择sigmod和tanh函数
    * sigmod求导：f(z) = 1 / (1 + exp(-z)) ----> f'(z) = f(z)(1 - f(z))
    * tanh求导：f(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z)) ----> f'(z) = 1 - (f(x))^2
* 神经网络模型
    * 概念：将多个单一神经元进行级联
    * 前向传播（计算神经网络的输出值）：从L1层开始，把第l层每个单元的激活值作为第l+1层的第i单元的权值来计算第l+1层各个单元的激活值
        * 向量化：z(l+1) = W(l)a(l) + b(l), a(l+1) = f(z(l+1)).

<h3 id="ch1.2">反向传导算法</h3>

<h3 id="ch1.3">TODO梯度校验与稀疏性</h3>

<h2 id="ch2">TODO矢量化编程实现</h2>
