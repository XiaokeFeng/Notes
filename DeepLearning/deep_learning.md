# Deep Learning

## Overview
* [稀疏自编码器](#ch1)
    * [神经网络](#ch1.1)
    * [反向传导算法](#ch1.2)
    * [梯度校验](#ch1.3)
    * [自编码算法与稀疏性](#ch1.4)
* [矢量化编程实现](#ch2)

<h2 id="ch1">稀疏自编码器</h2>

<h3 id="ch1.1">神经网络</h3>
* 概述
    * 概念：一个神经元，N个输入值，经过激活函数，得到一个输出值。
    * 激活函数：sigmod函数-->激活函数，则神经元为一个逻辑回归。
    * 如何选择激活函数：通常选择一些具有两个极值，且从一个极值能平滑过渡到另一个极值的函数。通常选择sigmod和tanh函数
    * sigmod求导：f(z) = 1 / (1 + exp(-z)) ----> f'(z) = f(z)(1 - f(z))
    * tanh求导：f(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z)) ----> f'(z) = 1 - (f(x))^2
* 神经网络模型
    * 概念：将多个单一神经元进行级联
    * 前向传播（计算神经网络的输出值）：从L1层开始，把第l层每个单元的激活值作为第l+1层的第i单元的权值来计算第l+1层各个单元的激活值
        * 向量化：z(l+1) = W(l)a(l) + b(l), a(l+1) = f(z(l+1)).

<h3 id="ch1.2">反向传导算法</h3>
* 概念：利用梯度下降求解cost function，用于迭代W和b
* 步骤
    * 前馈传导，计算各层的激活值
    * 计算输出层的残差
    * 根据输出层的残差，递归求解前一层的残差
    * 得到每一层的W和b的偏导
* 利用反向传导算法求出了每一层的偏导，再选择合适的learning rate，即可迭代神经网络模型

<h3 id="ch1.3">梯度校验</h3>
* 如何校验你的求导代码是否正确实现了？利用求导的定义来检验

<h3 id="ch1.4">自编码算法与稀疏性</h3>
* 一种无监督学习
* 函数：逼近一个恒等函数，使得输出值无限接近于输入值

<h2 id="ch2">TODO矢量化编程实现</h2>
